name: Run Benchmarks

on:
  workflow_dispatch:
    inputs:
      benchmark_file:
        description: 'Benchmark file to run'
        required: true
        type: choice
        options:
          - ValidationBenchmarks.cs
          - FluentFriendlyValidationBenchmarks.cs
          - CollectionBenchmarks.cs
          - DateTimeBenchmarks.cs
          - GeneralBenchmarks.cs
          - NumericBenchmarks.cs
          - OtherBenchmarks.cs
          - StringBenchmarks.cs

jobs:
  benchmark:
    runs-on: ubuntu-latest
    outputs:
      artifact-name: ${{ steps.upload.outputs.artifact-name }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Test local NuGet package
        run: ./test-local-nuget.sh
        
      - name: Restore ConsoleTest project
        run: dotnet restore Test/ConsoleTest/ConsoleTest.csproj

      - name: Build ConsoleTest project in Release mode
        run: dotnet build Test/ConsoleTest/ConsoleTest.csproj --configuration Release --no-restore

      - name: Run benchmarks
        run: |
          BENCHMARK_CLASS="${{ github.event.inputs.benchmark_file }}"
          BENCHMARK_CLASS="${BENCHMARK_CLASS%.cs}"
          dotnet run --project Test/ConsoleTest/ConsoleTest.csproj --configuration Release --no-build --runtimes net9.0 "$BENCHMARK_CLASS"

      - name: Copy benchmark result
        run: |
          mkdir -p benchmark-output
          echo "Available files:"
          ls -R BenchmarkDotNet.Artifacts || echo "No artifacts directory"
          if [ -d BenchmarkDotNet.Artifacts ]; then
            cp -r BenchmarkDotNet.Artifacts/* benchmark-output/
          else
            echo "No BenchmarkDotNet.Artifacts directory found"
          fi

      - name: Ensure CSV(s) are available for comparison
        run: |
          shopt -s nullglob
          CSVFILES=(benchmark-output/results/*.csv)
          if [ ${#CSVFILES[@]} -gt 0 ]; then
            for f in "${CSVFILES[@]}"; do
              cp "$f" benchmark-output/
              echo "Copied $f to benchmark-output root."
            done
          else
            echo "No CSV files found in results folder."
          fi

      - name: Upload benchmark result
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-${{ github.event.inputs.benchmark_file }}
          path: benchmark-output
          
  deploy-pages:
    needs: benchmark
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Download artifact from benchmark job
        uses: actions/download-artifact@v4
        with:
          name: benchmark-${{ github.event.inputs.benchmark_file }}
          path: ./site

      # Optional: create a simple landing page that links to all HTML reports
      - name: Generate index.html
        run: |
          echo "<!doctype html><meta charset='utf-8'>
          <title>EasyValidate Benchmarks</title>
          <h1>EasyValidate Benchmarks</h1>
          <p>Run: ${{ github.run_number }} â€¢ Commit: ${{ github.sha }}</p>
          <ul>" > ./site/index.html
          find ./site -type f -name "*.html" | sort | while read f; do
            rel=${f#./site/}
            echo "<li><a href='${rel}'>${rel}</a></li>" >> ./site/index.html
          done
          echo "</ul><hr>
          <p>CSV files are included for raw comparisons.</p>
          <p><a href='https://github.com/mu-dawood/EasyValidate/actions'>See all runs on GitHub Actions</a></p>" >> ./site/index.html
          touch ./site/.nojekyll  # ensures assets under folders like 'results' serve correctly

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
  compare:
    runs-on: ubuntu-latest
    needs: benchmark
    steps:
      - name: Install GitHub CLI
        run: sudo apt-get update && sudo apt-get install -y gh

      - name: Checkout repository
        uses: actions/checkout@v4


      - name: Get previous artifact ID for selected benchmark
        id: get_artifact_id
        run: |
          set -e
          BENCHMARK_ARTIFACT="benchmark-${{ github.event.inputs.benchmark_file }}"
          RUN_IDS=$(gh api \
            -H "Accept: application/vnd.github+json" \
            "/repos/${{ github.repository }}/actions/workflows/benchmarks.yml/runs?branch=${{ github.ref_name }}&status=success&per_page=50" \
            --jq ".workflow_runs[].id")
          ARTIFACT_ID=""
          for RUN_ID in $RUN_IDS; do
            ARTIFACT_ID=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/runs/${RUN_ID}/artifacts" \
              --jq ".artifacts[] | select(.name==\"${BENCHMARK_ARTIFACT}\") | .id" 2>&1 || true)
            if [ -n "$ARTIFACT_ID" ]; then
              break
            fi
          done
          if [ -z "$ARTIFACT_ID" ]; then
            echo "No previous artifact found for ${BENCHMARK_ARTIFACT}. Skipping comparison."
            echo "artifact_id=" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "artifact_id=$ARTIFACT_ID" >> $GITHUB_OUTPUT
        env:
          GH_TOKEN: ${{ secrets.GH_PAT }}


      - name: Download previous benchmark-current artifact
        if: ${{ steps.get_artifact_id.outputs.artifact_id != '' }}
        run: |
          gh api \
            -H "Accept: application/vnd.github+json" \
            "/repos/${{ github.repository }}/actions/artifacts/${{ steps.get_artifact_id.outputs.artifact_id }}/zip" > previous.zip
          unzip -o previous.zip -d previous
        env:
          GH_TOKEN: ${{ secrets.GH_PAT }} 

      - name: Download current result
        if: ${{ steps.get_artifact_id.outputs.artifact_id != '' }}
        uses: actions/download-artifact@v4
        with:
          name: benchmark-${{ github.event.inputs.benchmark_file }}
          path: current

      - name: Summarize benchmark diff with OpenAI
        if: ${{ steps.get_artifact_id.outputs.artifact_id != '' }}
        run: |
          pip install google-generativeai
          shopt -s nullglob
          PREV_CSV=(previous/*.csv)
          CUR_CSV=(current/*.csv)
          if [ ${#PREV_CSV[@]} -gt 0 ] && [ ${#CUR_CSV[@]} -gt 0 ]; then
            for prev in "${PREV_CSV[@]}"; do
              fname=$(basename "$prev")
              cur="current/$fname"
              if [ -f "$cur" ]; then
                GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }} python .github/scripts/ai_benchmark_diff.py "$prev" "$cur"
              fi
            done
          fi
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}


